{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58f6034",
   "metadata": {},
   "source": [
    "# Install packages\n",
    "\n",
    "https://wiki.postgresql.org/wiki/Using_psycopg2_with_PostgreSQL#Fetch_Records_using_a_Server-Side_Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3517f1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.2.0)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (24.1.0)\n",
      "Requirement already satisfied: neo4j in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (5.28.2)\n",
      "Collecting load_dotenv\n",
      "  Downloading load_dotenv-0.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: psycopg[binary] in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.2.10)\n",
      "Requirement already satisfied: backports.zoneinfo>=0.2.0 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from psycopg[binary]) (0.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from psycopg[binary]) (4.12.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from psycopg[binary]) (2024.2)\n",
      "Requirement already satisfied: psycopg-binary==3.2.10 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from psycopg[binary]) (3.2.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from openai) (4.5.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from openai) (0.9.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from neo4j) (2024.2)\n",
      "Collecting python-dotenv (from load_dotenv)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: colorama in c:\\users\\hayley\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm>4->openai) (0.4.3)\n",
      "Downloading load_dotenv-0.1.0-py3-none-any.whl (7.2 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, load_dotenv\n",
      "\n",
      "Successfully installed load_dotenv-0.1.0 python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "pip install \"psycopg[binary]\" openai aiofiles neo4j load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4832eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_dotenv\n",
    "import pandas as pd\n",
    "import psycopg                      # SQL query package\n",
    "import os                           # get env variables\n",
    "from urllib.request import urlopen  # package for HTTP connections\n",
    "import time\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d63f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = load_dotenv(\"DB_HOST\")\n",
    "DB_NAME = load_dotenv(\"DB_NAME\")\n",
    "DB_USER = load_dotenv(\"DB_USER\")\n",
    "DB_PASS = load_dotenv(\"DB_PASS\")\n",
    "\n",
    "OPENAI_KEY = load_dotenv(\"OPENAI_KEY\")\n",
    "OPENAI_MODEL = load_dotenv(\"OPENAI_MODEL\")\n",
    "\n",
    "NEO4J_URI = load_dotenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = load_dotenv(\"NEO4J_USER\")\n",
    "NEO4J_PASS = load_dotenv(\"NEO4J_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a171191",
   "metadata": {},
   "source": [
    "# Connect to Postgres\n",
    "creates `list_of_ids` = a list of document IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = f\"host='{DB_HOST}' dbname = '{DB_NAME}' user='{DB_USER}' password='{DB_PASS}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec4aa7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg.connect(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a957e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE IS THE IMPORTANT PART, by specifying a name for the cursor\n",
    "# psycopg2 creates a server-side cursor, which prevents all of the records from being downloaded at once from the server.\n",
    "cursor = conn.cursor('server_cursor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef164ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT id FROM ucsf_opioid.table_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1416a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(query)\n",
    "records = cursor.fetchall()\n",
    "print(len(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d95743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4282450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'htcf0232'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each record and save data into dictionary\n",
    "list_of_ids = []\n",
    "\n",
    "for i in records:\n",
    "    list_of_ids.append(i[0])\n",
    "list_of_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b88a5",
   "metadata": {},
   "source": [
    "# Connect to Solr\n",
    "creates `emails_list` = list of dictionaries \n",
    "* {\"email_id\": email_id, \"email_text\": email_body}\n",
    "\n",
    "outputs `email_bodies_list.csv` of all ids and their email body texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the email body from Solr to the metadata\n",
    "emails_list = []\n",
    "for i in list_of_ids:\n",
    "    id_and_body = {}\n",
    "    record_id = i\n",
    "\n",
    "    id_and_body[\"email_id\"] = record_id\n",
    "\n",
    "    solr_url = f'{record_id}'\n",
    "    connection = urlopen(solr_url)\n",
    "\n",
    "    response_text = connection.read().decode('utf-8')\n",
    "    data = json.loads(response_text)\n",
    "    email_body = data['response']['docs'][0]['ocr_text'][0]\n",
    "    \n",
    "    id_and_body[\"email_text\"] = email_body\n",
    "    emails_list.append(id_and_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4386698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# email_bodies_list.csv has list of dictionaries {email_id: email_body}\n",
    "with open(\"email_bodies_list.csv\", 'w', newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    for i in emails_list:\n",
    "        csv_writer.writerow([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc9607",
   "metadata": {},
   "source": [
    "# Call Batch API and OpenAI API\n",
    "to format bodies into the schema by:\n",
    "\n",
    "defining schema and system_instructions\n",
    "\n",
    "checkpoint handling for in case the system crashes halfway through\n",
    "\n",
    "outputs `checkpoints.jsonl` and `output_list` within python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5608f06",
   "metadata": {},
   "source": [
    "## schema\n",
    "`schema_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d135c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "  \"@context\": {\n",
    "    \"@vocab\": \"https://schema.org/\",\n",
    "    \"email\": \"https://schema.org/EmailMessage\",\n",
    "    \"person\": \"https://schema.org/Person\",\n",
    "    \"org\": \"https://schema.org/Organization\",\n",
    "    \"document\": \"https://schema.org/DigitalDocument\",\n",
    "    \"topicEntity\": \"https://schema.org/Thing\",\n",
    "    \"url\": \"https://schema.org/URL\",\n",
    "    \"gpe\": \"https://schema.org/Place\",\n",
    "    \"drug\": \"https://schema.org/Drug\"\n",
    "  },\n",
    "\n",
    "  # Thread-level wrapper\n",
    "  \"@type\": \"case:Legislation\",\n",
    "  \"semantic_type\": \"Legal Communication Record\",\n",
    "  \"identifier\": \"\",\n",
    "  \"legalStatus\": \"\",\n",
    "  \"dateFiled\": \"\",\n",
    "  \"language\": [],   # could be \"en\" for English or a different language\n",
    "  \"confidentialityNotice\": \"\",\n",
    "\n",
    "  # MUST be the full raw CSV cell, unchanged except for JSON escaping\n",
    "#   \"raw_thread_text\": \"\",\n",
    "\n",
    "  # The thread: multiple messages in reverse-chronological order\n",
    "  \"hasPart\": [\n",
    "    {\n",
    "      \"@type\": \"email:EmailMessage\",\n",
    "      \"semantic_type\": \"Email Communication\",\n",
    "\n",
    "      \"identifier\": \"\",\n",
    "      \"subject\": \"\",      # substring of raw_thread_text (if present)\n",
    "      \"dateSent\": \"\",\n",
    "      \"importance\": \"\",\n",
    "\n",
    "      \"threadIndex\": 0,   # 0 = most recent message\n",
    "      \"inReplyTo\": \"\",    # optional\n",
    "\n",
    "      \"sender\": {\n",
    "        \"@type\": \"person:Person\",\n",
    "        \"semantic_type\": \"Person\",\n",
    "        \"name\": \"\",       # substring of raw_thread_text\n",
    "        \"email\": \"\",      # substring of raw_thread_text\n",
    "        \"affiliation\": {\n",
    "          \"@type\": \"org:Organization\",\n",
    "          \"semantic_type\": \"ORG\",\n",
    "          \"name\": \"\",     # substring if present, else empty\n",
    "          \"role\": \"\",\n",
    "          \"parentOrganization\": {\n",
    "            \"@type\": \"org:Organization\",\n",
    "            \"semantic_type\": \"ORG\",\n",
    "            \"name\": \"\",\n",
    "            \"role\": \"\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "\n",
    "      \"recipient\": [\n",
    "        {\n",
    "          \"@type\": \"person:Person\",\n",
    "          \"semantic_type\": \"Person\",\n",
    "          \"name\": \"\",     # substring\n",
    "          \"email\": \"\",    # substring if present\n",
    "          \"affiliation\": {\n",
    "            \"@type\": \"org:Organization\",\n",
    "            \"semantic_type\": \"ORG\",\n",
    "            \"name\": \"\",\n",
    "            \"role\": \"\"\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "\n",
    "      # MUST be a literal substring of raw_thread_text\n",
    "      \"body\": \"\",\n",
    "\n",
    "      # MENTIONS: text grounded, labels inferred\n",
    "      \"mentions\": [\n",
    "        {\n",
    "          \"@type\": \"topicEntity\",\n",
    "          \"semantic_type\": \"\",   # e.g. \"Legal Case\", \"Product Brand\", \"Business Operation\", \"Financial Document\", \"Drug Name\", \"GPE\"\n",
    "          \"role\": \"\",            # e.g. \"Geographic Destination\"\n",
    "          # MUST be substring of the user input text\n",
    "          \"name\": \"\",\n",
    "          # MUST be substring if it appears in the text; otherwise leave \"\"\n",
    "          \"identifier\": \"\"\n",
    "        }\n",
    "      ],\n",
    "\n",
    "      # ATTACHMENTS: names/desc grounded, format may be inferred\n",
    "      \"attachments\": [\n",
    "        {\n",
    "          \"@type\": \"document:DigitalDocument\",\n",
    "          \"semantic_type\": \"\",   # e.g. \"Spreadsheet Document\", \"Presentation Document\", \"Policy Document\"\n",
    "\n",
    "          # MUST be substring of raw_thread_text\n",
    "          \"name\": \"\",            # e.g. \"TLAIS costings 8.09.04.xls\"\n",
    "\n",
    "          # fileFormat may be inferred (application/pdf, etc.)\n",
    "          \"fileFormat\": \"\",\n",
    "\n",
    "          # MUST be substring if the description text appears in the email;\n",
    "          # otherwise either leave \"\" or omit the field.\n",
    "          \"description\": \"\"\n",
    "        }\n",
    "      ],\n",
    "\n",
    "      \"forwardedMessage\": None,\n",
    "      \"mentionsEmail\": [],\n",
    "      \"structuredArgument\": [],\n",
    "      \"complianceContext\": {\n",
    "          \"@type\": \"CreativeWork\",\n",
    "          \"semantic_type\": \"\",\n",
    "          \"name\": \"\",\n",
    "          \"keywords\": [],\n",
    "          \"about\": []\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "schema_json = json.dumps(schema, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda698e",
   "metadata": {},
   "source": [
    "## continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7466ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instructions = f\"\"\"\n",
    "- The user input is a single email thread (one CSV row).\n",
    "- Split the thread into individual email messages.\n",
    "- For each message, create one object in `hasPart` with @type \"email:EmailMessage\".\n",
    "- Order `hasPart` in reverse-chronological order so index 0 is the most recent message.\n",
    "\n",
    "GROUNDING RULES (VERY IMPORTANT):\n",
    "- `body` MUST be a literal substring of the user input text. Do not paraphrase or summarize.\n",
    "- `subject`, sender/recipient `name` and `email`, attachment `name`, and any description text\n",
    "  MUST be literal substrings of the user input text where they appear.\n",
    "- For each `mentions` item:\n",
    "  - `name` MUST be a literal substring of the user input text.\n",
    "  - `identifier` MUST be a literal substring of the user input text if present; otherwise leave \"\".\n",
    "  - `semantic_type` and `role` are categorical labels and MAY be inferred.\n",
    "- For each `attachments` item:\n",
    "  - `name` MUST be a literal substring of the user input text.\n",
    "  - `description` MUST be a literal substring if such text exists; otherwise use \"\" or omit the field.\n",
    "  - `fileFormat` MAY be inferred (e.g., \"application/vnd.ms-excel\" for .xls) and\n",
    "    does not need to match any literal span.\n",
    "- Do NOT introduce any text in `body`, `name`, `description`, `subject`,\n",
    "  or other free-text fields that is not a substring of the user input text.\n",
    "\n",
    "- You may assign or infer categorical labels such as:\n",
    "  - mentions.semantic_type: \"Legal Case\", \"Product Brand\", \"Business Operation\",\n",
    "    \"Financial Document\", \"Drug Name\", \"GPE\", etc.\n",
    "  - mentions.role: e.g., \"Geographic Destination\".\n",
    "  - attachments.semantic_type: \"Spreadsheet Document\", \"Presentation Document\",\n",
    "    \"Policy Document\", etc.\n",
    "  - attachments.fileFormat: e.g., \"application/vnd.ms-excel\", \"application/pdf\", etc.\n",
    "  - complianceContext.semantic_type: e.g., \"Regulatory and Legal Framework\".\n",
    "  - complianceContext.name: e.g., \"Export Control and Restricted Destination Compliance\".\n",
    "\n",
    "- Do NOT summarize the thread; your job is to structure it, not rewrite it.\n",
    "\n",
    "- Output strictly must be valid JSON only (no markdown or commentary).\n",
    "- Follow this JSON structure exactly: {schema_json}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcc40e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(lst, size):\n",
    "    for i in range(0, len(lst), size):\n",
    "        yield lst[i:i+size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch_jsonl(\n",
    "    emails: List[Dict[str, str]],\n",
    "    system_instructions: str,\n",
    "    jsonl_path: str,\n",
    "    model: str = OPENAI_MODEL,  # or any chat model with JSON mode\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build a JSONL file where each line is a Batch request for /v1/chat/completions,\n",
    "    keyed by email_id via custom_id.\n",
    "    \"\"\"\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in emails:\n",
    "            email_id = rec[\"email_id\"]\n",
    "            email_text = rec[\"email_text\"]\n",
    "\n",
    "            task = {\n",
    "                \"custom_id\": str(email_id),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"response_format\": {\"type\": \"json_object\"},\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"max_tokens\": 6000,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_instructions,\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": email_text,\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(task, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d4babb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_batch(jsonl_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Upload the batch JSONL file and create a Batch job.\n",
    "    Returns the batch_id.\n",
    "    \"\"\"\n",
    "    # 1. Upload file\n",
    "    with open(jsonl_path, \"rb\") as f:\n",
    "        batch_file = client.files.create(\n",
    "            file=f,\n",
    "            purpose=\"batch\",\n",
    "        )\n",
    "\n",
    "    # 2. Create batch job\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",  # currently the only allowed window\n",
    "        metadata={\"job_type\": \"ocr_email_structuring\"},\n",
    "    )\n",
    "\n",
    "    print(\"Created batch:\", batch.id)\n",
    "    return batch.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c34e0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_batch(batch_id: str, poll_interval: int = 60, max_polls: int = 120):\n",
    "    \"\"\"\n",
    "    Polls the batch until it reaches a terminal state or until max_polls is exceeded.\n",
    "    Returns the last seen batch object.\n",
    "    \"\"\"\n",
    "    polls = 0\n",
    "    last_completed = None\n",
    "\n",
    "    while True:\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        polls += 1\n",
    "\n",
    "        print(\n",
    "            f\"Batch {batch_id} status: {batch.status} | \"\n",
    "            f\"completed={batch.request_counts.completed} / total={batch.request_counts.total}\"\n",
    "        )\n",
    "\n",
    "        # Track progress; if you want, detect \"stuck\" here\n",
    "        if last_completed is None or batch.request_counts.completed != last_completed:\n",
    "            last_completed = batch.request_counts.completed\n",
    "\n",
    "        if batch.status in (\"completed\", \"failed\", \"expired\", \"cancelled\"):\n",
    "            return batch\n",
    "\n",
    "        if polls >= max_polls:\n",
    "            print(f\"Reached max_polls={max_polls}, stopping wait loop.\")\n",
    "            return batch\n",
    "\n",
    "        time.sleep(poll_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5116cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_parse_results(\n",
    "    batch,\n",
    "    output_jsonl_path: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a completed batch object, download its output JSONL and return:\n",
    "        { custom_id (email_id): parsed_json }\n",
    "\n",
    "    If output_jsonl_path is given, append each as:\n",
    "        {\"email_id\": \"<id>\", \"output\": <parsed_json>}\n",
    "    to that JSONL file.\n",
    "    \"\"\"\n",
    "    if not batch.output_file_id:\n",
    "        raise RuntimeError(f\"Batch {batch.id} has no output_file_id; status={batch.status}\")\n",
    "\n",
    "    file_content = client.files.content(batch.output_file_id).text\n",
    "\n",
    "    results_by_email_id: Dict[str, Any] = {}\n",
    "\n",
    "    out_f = open(output_jsonl_path, \"a\", encoding=\"utf-8\") if output_jsonl_path else None\n",
    "\n",
    "    try:\n",
    "        for line in file_content.splitlines():\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            record = json.loads(line)\n",
    "            custom_id = record.get(\"custom_id\")\n",
    "\n",
    "            # 1) Batch-level error for this task\n",
    "            if record.get(\"error\") is not None:\n",
    "                parsed_json = {\"_error\": record[\"error\"]}\n",
    "                results_by_email_id[custom_id] = parsed_json\n",
    "                if out_f is not None:\n",
    "                    out_f.write(json.dumps({\"email_id\": custom_id, \"output\": parsed_json},\n",
    "                                           ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            response = record.get(\"response\")\n",
    "            if response is None:\n",
    "                # Unexpected shape\n",
    "                parsed_json = {\"_raw_record\": record}\n",
    "                results_by_email_id[custom_id] = parsed_json\n",
    "                if out_f is not None:\n",
    "                    out_f.write(json.dumps({\"email_id\": custom_id, \"output\": parsed_json},\n",
    "                                           ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            body = response.get(\"body\", {})\n",
    "            choices = body.get(\"choices\") or []\n",
    "            if not choices:\n",
    "                parsed_json = {\"_raw_body\": body}\n",
    "                results_by_email_id[custom_id] = parsed_json\n",
    "                if out_f is not None:\n",
    "                    out_f.write(json.dumps({\"email_id\": custom_id, \"output\": parsed_json},\n",
    "                                           ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            choice0 = choices[0]\n",
    "            message = choice0.get(\"message\", {})\n",
    "            finish_reason = choice0.get(\"finish_reason\")\n",
    "            content = message.get(\"content\")\n",
    "\n",
    "            # If we hit the max_tokens limit, flag it explicitly\n",
    "            if finish_reason == \"length\":\n",
    "                parsed_json = {\n",
    "                    \"_error\": \"truncated_output_max_tokens\",\n",
    "                    \"_raw_content\": content,\n",
    "                }\n",
    "                results_by_email_id[custom_id] = parsed_json\n",
    "                if out_f is not None:\n",
    "                    out_f.write(json.dumps({\"email_id\": custom_id, \"output\": parsed_json},\n",
    "                                           ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            # 2) Normalize content into a string\n",
    "            if isinstance(content, list):\n",
    "                parts = []\n",
    "                for block in content:\n",
    "                    if isinstance(block, dict):\n",
    "                        if \"text\" in block and isinstance(block[\"text\"], str):\n",
    "                            parts.append(block[\"text\"])\n",
    "                        elif \"output_text\" in block and isinstance(block[\"output_text\"], dict):\n",
    "                            t = block[\"output_text\"].get(\"text\")\n",
    "                            if isinstance(t, str):\n",
    "                                parts.append(t)\n",
    "                content_str = \"\".join(parts).strip()\n",
    "            elif isinstance(content, str):\n",
    "                content_str = content.strip()\n",
    "            else:\n",
    "                parsed_json = {\"_raw_content\": content}\n",
    "                results_by_email_id[custom_id] = parsed_json\n",
    "                if out_f is not None:\n",
    "                    out_f.write(json.dumps({\"email_id\": custom_id, \"output\": parsed_json},\n",
    "                                           ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            # 3) Parse JSON content\n",
    "            try:\n",
    "                parsed_json = json.loads(content_str)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"FAILED TO PARSE JSON FOR\", custom_id)\n",
    "                print(\"Content snippet:\", repr(content_str[:300]))\n",
    "                parsed_json = {\"_raw_content\": content_str}\n",
    "\n",
    "            results_by_email_id[custom_id] = parsed_json\n",
    "\n",
    "            if out_f is not None:\n",
    "                out_f.write(json.dumps({\"email_id\": custom_id, \"output\": parsed_json},\n",
    "                                       ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    finally:\n",
    "        if out_f is not None:\n",
    "            out_f.close()\n",
    "\n",
    "    return results_by_email_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20992072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl_path = \"OpenAI_API_Output.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = []\n",
    "for i, email_chunk in enumerate(chunk(emails_list, 300)):\n",
    "    jsonl_path = f\"batch_input_{i:04d}.jsonl\"\n",
    "    build_batch_jsonl(email_chunk, system_instructions, jsonl_path)\n",
    "    batch_id = submit_batch(jsonl_path)\n",
    "    batch_ids.append(batch_id)\n",
    "    final_batch = wait_for_batch(batch_id, poll_interval=60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_jsonl = \"OpenAI_API_Output.jsonl\"\n",
    "\n",
    "for batch_id in batch_ids:\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    if batch.status != \"completed\":\n",
    "        print(f\"Skipping batch {batch_id} with status={batch.status}\")\n",
    "        continue\n",
    "\n",
    "    download_and_parse_results(batch, output_jsonl_path=combined_jsonl)\n",
    "\n",
    "print(f\"Combined output written to {combined_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_jsonl = \"OpenAI_API_Output.jsonl\"\n",
    "\n",
    "# Start with a fresh/truncated file (optional)\n",
    "# open(combined_jsonl, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "# List batches from the API (tweak limit as needed)\n",
    "batch_list = client.batches.list(limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c32822",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_list.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece24b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batch_list.data:\n",
    "    # Filter to your specific job type, and only completed ones\n",
    "    if getattr(batch, \"metadata\", None) and batch.metadata.get(\"job_type\") == \"ocr_email_structuring\":\n",
    "        if batch.status != \"completed\":\n",
    "            print(f\"Skipping batch {batch.id} with status={batch.status}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Collecting from batch {batch.id}\")\n",
    "        download_and_parse_results(batch, output_jsonl_path=combined_jsonl)\n",
    "\n",
    "print(f\"Combined output written to {combined_jsonl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build JSONL batch file\n",
    "build_batch_jsonl(\n",
    "    emails=emails_list,\n",
    "    system_instructions=system_instructions,\n",
    "    jsonl_path=jsonl_path,\n",
    "    model=OPENAI_MODEL,   # or your preferred model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c339f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit batch\n",
    "batch_id = submit_batch(jsonl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f37281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for completion\n",
    "final_batch = wait_for_batch(batch_id, poll_interval=30)\n",
    "\n",
    "if final_batch.status != \"completed\":\n",
    "    print(\"Batch did not complete successfully.\")\n",
    "    print(\"Status:\", final_batch.status)\n",
    "    print(\"Errors:\", getattr(final_batch, \"errors\", None))\n",
    "    print(\"Request counts:\", final_batch.request_counts)\n",
    "\n",
    "    if final_batch.output_file_id:\n",
    "        print(\"Attempting to inspect output file for per-request errors...\")\n",
    "        out_content = client.files.content(final_batch.output_file_id).text\n",
    "        for i, line in enumerate(out_content.splitlines()):\n",
    "            print(line)\n",
    "            if i >= 20:  # show first 20 lines max\n",
    "                break\n",
    "    else:\n",
    "        print(\"No output_file_id present.\")\n",
    "\n",
    "    # Only raise after logging everything:\n",
    "    raise RuntimeError(f\"Batch failed/ended with status={final_batch.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and parse structured results\n",
    "structured_by_email_id = download_and_parse_results(final_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e454a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: inspect one\n",
    "for email_id, structured in list(structured_by_email_id.items())[:3]:\n",
    "    print(f\"\\n=== {email_id} ===\")\n",
    "    print(json.dumps(structured, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edabc0",
   "metadata": {},
   "source": [
    "# The code below this cell is depreciated.\n",
    "Please refer to the `Part 3` notebook for the code that connects to Neo4j and to `graph_queries.py` for the Cypher queries to build the knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa304c5",
   "metadata": {},
   "source": [
    "# Connect to Neo4j\n",
    "https://browser.neo4j.io/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a47776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO CLOSE CONNECTION\n",
    "uri = NEO4J_URI\n",
    "username = NEO4J_USER\n",
    "password = NEO4J_PASS\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01051d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record n=1>\n"
     ]
    }
   ],
   "source": [
    "# test query\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"RETURN 1 AS n\")\n",
    "    print(result.single())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ceaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# email subject lines and drug name of any email that mentions drugs\n",
    "with driver.session() as session:\n",
    "    cypher_query = session.run('MATCH (e:Email)-[:MENTIONS]->(t:TopicEntity {semantic_type: \"Drug Name\"}) RETURN e.subject, t.name;')\n",
    "    print(type(cypher_query))\n",
    "    cypher_query = cypher_query.data()   # converts from Result to list\n",
    "for i in cypher_query:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf034f",
   "metadata": {},
   "source": [
    "## add constraints\n",
    "`setup_constraints` with parameters (uri, user, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_constraints(uri, user, password):\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    constraint_statements = [\n",
    "        # Core entities\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT case_identifier IF NOT EXISTS\n",
    "        FOR (c:Case)\n",
    "        REQUIRE c.identifier IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT email_identifier IF NOT EXISTS\n",
    "        FOR (e:Email)\n",
    "        REQUIRE e.identifier IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT person_key IF NOT EXISTS\n",
    "        FOR (p:Person)\n",
    "        REQUIRE p.key IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT org_name IF NOT EXISTS\n",
    "        FOR (o:Organization)\n",
    "        REQUIRE o.name IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT document_name IF NOT EXISTS\n",
    "        FOR (d:Document)\n",
    "        REQUIRE d.name IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT place_name IF NOT EXISTS\n",
    "        FOR (pl:Place)\n",
    "        REQUIRE pl.name IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT topicentity_name IF NOT EXISTS\n",
    "        FOR (t:TopicEntity)\n",
    "        REQUIRE t.name IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT crossrefemail_cid IF NOT EXISTS\n",
    "        FOR (x:CrossRefEmail)\n",
    "        REQUIRE x.cid IS UNIQUE\n",
    "        \"\"\",\n",
    "\n",
    "        # Enriched-content entities\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT decision_text IF NOT EXISTS\n",
    "        FOR (d:Decision)\n",
    "        REQUIRE d.text IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT concern_text IF NOT EXISTS\n",
    "        FOR (c:Concern)\n",
    "        REQUIRE c.text IS UNIQUE\n",
    "        \"\"\",\n",
    "\n",
    "        # FinancialMention â€“ there are two flavors, so we use two constraints:\n",
    "        # one for simple text mentions, one for (description, figure, currency)\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT financialmention_text IF NOT EXISTS\n",
    "        FOR (f:FinancialMention)\n",
    "        REQUIRE f.text IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT financialmention_desc_fig_cur IF NOT EXISTS\n",
    "        FOR (f:FinancialMention)\n",
    "        REQUIRE (f.description, f.figure, f.currency) IS UNIQUE\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    with driver.session() as session:\n",
    "        for stmt in constraint_statements:\n",
    "            session.run(stmt)\n",
    "\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabcaf8",
   "metadata": {},
   "source": [
    "## cypher queries\n",
    "creates function `import_jsonl_to_neo4j`\n",
    "with parameters(json_path, uri, user, password, log_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "def ensure_list(x: Union[None, Dict[str, Any], List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return [x]\n",
    "\n",
    "\n",
    "# ----------------- Upsert helpers ----------------- #\n",
    "\n",
    "def upsert_case(tx, case_obj: Dict[str, Any]):\n",
    "    case_id = case_obj.get(\"identifier\")\n",
    "    if not case_id:\n",
    "        return\n",
    "\n",
    "    tx.run(\n",
    "        \"\"\"\n",
    "        MERGE (c:Case {identifier: $identifier})\n",
    "        SET\n",
    "          c.semantic_type = $semantic_type,\n",
    "          c.legalStatus = $legalStatus,\n",
    "          c.dateFiled = $dateFiled,\n",
    "          c.confidentialityNotice = $confidentialityNotice,\n",
    "          c.language = $language\n",
    "        \"\"\",\n",
    "        identifier=case_id,\n",
    "        semantic_type=case_obj.get(\"semantic_type\"),\n",
    "        legalStatus=case_obj.get(\"legalStatus\"),\n",
    "        dateFiled=case_obj.get(\"dateFiled\"),\n",
    "        confidentialityNotice=case_obj.get(\"confidentialityNotice\"),\n",
    "        language=case_obj.get(\"language\"),\n",
    "    )\n",
    "\n",
    "    # Case-level mentions\n",
    "    for mention in case_obj.get(\"mentions\") or []:\n",
    "        if isinstance(mention, dict):\n",
    "            upsert_case_mention(tx, case_id, mention)\n",
    "\n",
    "    # hasPart emails\n",
    "    for email_obj in ensure_list(case_obj.get(\"hasPart\")):\n",
    "        if isinstance(email_obj, dict):\n",
    "            upsert_email_recursive(tx, case_id, email_obj, parent_email_id=None)\n",
    "\n",
    "\n",
    "def upsert_case_mention(tx, case_id: str, mention: Dict[str, Any]):\n",
    "    m_type = mention.get(\"@type\")\n",
    "    name = mention.get(\"name\")\n",
    "    if not name:\n",
    "        return\n",
    "\n",
    "    sem = mention.get(\"semantic_type\")\n",
    "    identifier = mention.get(\"identifier\")\n",
    "\n",
    "    if m_type == \"gpe\":\n",
    "        label = \"Place\"\n",
    "    else:\n",
    "        label = \"TopicEntity\"\n",
    "\n",
    "    # Node\n",
    "    tx.run(\n",
    "        f\"\"\"\n",
    "        MERGE (m:{label} {{name: $name}})\n",
    "        SET\n",
    "          m.semantic_type = $semantic_type,\n",
    "          m.identifier = $identifier\n",
    "        \"\"\",\n",
    "        name=name,\n",
    "        semantic_type=sem,\n",
    "        identifier=identifier,\n",
    "    )\n",
    "\n",
    "    # Relationship\n",
    "    tx.run(\n",
    "        f\"\"\"\n",
    "        MATCH (c:Case {{identifier: $case_id}})\n",
    "        MATCH (m:{label} {{name: $name}})\n",
    "        MERGE (c)-[:CASE_MENTIONS]->(m)\n",
    "        \"\"\",\n",
    "        case_id=case_id,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "def upsert_person(tx, person: Dict[str, Any]) -> str:\n",
    "    if not person:\n",
    "        return None\n",
    "\n",
    "    name = person.get(\"name\") or \"Unknown\"\n",
    "    email_addr = person.get(\"email\")\n",
    "    sem = person.get(\"semantic_type\")\n",
    "    key = email_addr or name\n",
    "\n",
    "    tx.run(\n",
    "        \"\"\"\n",
    "        MERGE (p:Person {key: $key})\n",
    "        SET\n",
    "          p.name = $name,\n",
    "          p.email = $email,\n",
    "          p.semantic_type = $semantic_type\n",
    "        \"\"\",\n",
    "        key=key,\n",
    "        name=name,\n",
    "        email=email_addr,\n",
    "        semantic_type=sem,\n",
    "    )\n",
    "\n",
    "    aff = person.get(\"affiliation\")\n",
    "    if isinstance(aff, dict):\n",
    "        upsert_org_for_person(tx, key, aff)\n",
    "\n",
    "    return key\n",
    "\n",
    "\n",
    "def upsert_org_for_person(tx, person_key: str, org: Dict[str, Any]):\n",
    "    name = org.get(\"name\")\n",
    "    if not name:\n",
    "        return\n",
    "\n",
    "    role = org.get(\"role\")\n",
    "    sem = org.get(\"semantic_type\")\n",
    "\n",
    "    # Org node\n",
    "    tx.run(\n",
    "        \"\"\"\n",
    "        MERGE (o:Organization {name: $name})\n",
    "        SET\n",
    "          o.semantic_type = $semantic_type,\n",
    "          o.role = $role\n",
    "        \"\"\",\n",
    "        name=name,\n",
    "        semantic_type=sem,\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    # Person -> Org\n",
    "    tx.run(\n",
    "        \"\"\"\n",
    "        MATCH (p:Person {key: $person_key})\n",
    "        MATCH (o:Organization {name: $name})\n",
    "        MERGE (p)-[:AFFILIATED_WITH]->(o)\n",
    "        \"\"\",\n",
    "        person_key=person_key,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    parent = org.get(\"parentOrganization\")\n",
    "    if isinstance(parent, dict) and parent.get(\"name\"):\n",
    "        pname = parent.get(\"name\")\n",
    "        psem = parent.get(\"semantic_type\")\n",
    "        prole = parent.get(\"role\")\n",
    "\n",
    "        # Parent org\n",
    "        tx.run(\n",
    "            \"\"\"\n",
    "            MERGE (po:Organization {name: $pname})\n",
    "            SET\n",
    "              po.semantic_type = $p_sem,\n",
    "              po.role = $p_role\n",
    "            \"\"\",\n",
    "            pname=pname,\n",
    "            p_sem=psem,\n",
    "            p_role=prole,\n",
    "        )\n",
    "\n",
    "        # Org -> Parent\n",
    "        tx.run(\n",
    "            \"\"\"\n",
    "            MATCH (o:Organization {name: $name})\n",
    "            MATCH (po:Organization {name: $pname})\n",
    "            MERGE (o)-[:SUBSIDIARY_OF]->(po)\n",
    "            \"\"\",\n",
    "            name=name,\n",
    "            pname=pname,\n",
    "        )\n",
    "\n",
    "\n",
    "def upsert_mention_for_email(tx, email_id: str, mention: Dict[str, Any]):\n",
    "    m_type = mention.get(\"@type\")\n",
    "    name = mention.get(\"name\")\n",
    "    if not name:\n",
    "        return\n",
    "\n",
    "    sem = mention.get(\"semantic_type\")\n",
    "    identifier = mention.get(\"identifier\")\n",
    "    role = mention.get(\"role\")\n",
    "\n",
    "    if m_type == \"gpe\":\n",
    "        label = \"Place\"\n",
    "        rel_type = \"EMAIL_MENTIONS_PLACE\"\n",
    "    else:\n",
    "        label = \"TopicEntity\"\n",
    "        rel_type = \"EMAIL_MENTIONS_TOPIC\"\n",
    "\n",
    "    # Node\n",
    "    tx.run(\n",
    "        f\"\"\"\n",
    "        MERGE (m:{label} {{name: $name}})\n",
    "        SET\n",
    "          m.semantic_type = $semantic_type,\n",
    "          m.identifier = $identifier,\n",
    "          m.role = $role\n",
    "        \"\"\",\n",
    "        name=name,\n",
    "        semantic_type=sem,\n",
    "        identifier=identifier,\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    # Relationship\n",
    "    tx.run(\n",
    "        f\"\"\"\n",
    "        MATCH (e:Email {{identifier: $email_id}})\n",
    "        MATCH (m:{label} {{name: $name}})\n",
    "        MERGE (e)-[:{rel_type}]->(m)\n",
    "        \"\"\",\n",
    "        email_id=email_id,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "def upsert_attachment(tx, email_id: str, case_id: str, attachment: Dict[str, Any]):\n",
    "    name = attachment.get(\"name\")\n",
    "    if not name:\n",
    "        return\n",
    "\n",
    "    sem = attachment.get(\"semantic_type\")\n",
    "    file_format = attachment.get(\"fileFormat\")\n",
    "    desc = attachment.get(\"description\")\n",
    "\n",
    "    # Document node\n",
    "    tx.run(\n",
    "        \"\"\"\n",
    "        MERGE (d:Document {name: $name})\n",
    "        SET\n",
    "          d.semantic_type = $semantic_type,\n",
    "          d.fileFormat = $fileFormat,\n",
    "          d.description = $description\n",
    "        \"\"\",\n",
    "        name=name,\n",
    "        semantic_type=sem,\n",
    "        fileFormat=file_format,\n",
    "        description=desc,\n",
    "    )\n",
    "\n",
    "    # Emailâ€“Document\n",
    "    tx.run(\n",
    "        \"\"\"\n",
    "        MATCH (e:Email {identifier: $email_id})\n",
    "        MATCH (d:Document {name: $name})\n",
    "        MERGE (e)-[:HAS_ATTACHMENT]->(d)\n",
    "        \"\"\",\n",
    "        email_id=email_id,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    # Caseâ€“Document\n",
    "    if case_id:\n",
    "        tx.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Case {identifier: $case_id})\n",
    "            MATCH (d:Document {name: $name})\n",
    "            MERGE (c)-[:CASE_HAS_DOCUMENT]->(d)\n",
    "            \"\"\",\n",
    "            case_id=case_id,\n",
    "            name=name,\n",
    "        )\n",
    "\n",
    "\n",
    "def upsert_email_recursive(tx, case_id: str, email_obj: Dict[str, Any], parent_email_id: str = None):\n",
    "    if not email_obj:\n",
    "        return\n",
    "\n",
    "    email_id = email_obj.get(\"identifier\")\n",
    "    if not email_id:\n",
    "        email_id = f\"{email_obj.get('subject', 'Unknown')}|{email_obj.get('dateSent', '')}\"\n",
    "\n",
    "    # Email node\n",
    "    tx.run(\n",
    "        \"\"\"\n",
    "        MERGE (e:Email {identifier: $identifier})\n",
    "        SET\n",
    "          e.semantic_type = $semantic_type,\n",
    "          e.subject = $subject,\n",
    "          e.dateSent = $dateSent,\n",
    "          e.importance = $importance,\n",
    "          e.body = $body\n",
    "        \"\"\",\n",
    "        identifier=email_id,\n",
    "        semantic_type=email_obj.get(\"semantic_type\"),\n",
    "        subject=email_obj.get(\"subject\"),\n",
    "        dateSent=email_obj.get(\"dateSent\"),\n",
    "        importance=email_obj.get(\"importance\"),\n",
    "        body=email_obj.get(\"body\"),\n",
    "    )\n",
    "\n",
    "    # Caseâ€“Email\n",
    "    if case_id:\n",
    "        tx.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Case {identifier: $case_id})\n",
    "            MATCH (e:Email {identifier: $email_id})\n",
    "            MERGE (c)-[:HAS_EMAIL]->(e)\n",
    "            \"\"\",\n",
    "            case_id=case_id,\n",
    "            email_id=email_id,\n",
    "        )\n",
    "\n",
    "    # Parent email relationship (for forwarded messages)\n",
    "    if parent_email_id:\n",
    "        tx.run(\n",
    "            \"\"\"\n",
    "            MATCH (parent:Email {identifier: $parent_id})\n",
    "            MATCH (child:Email {identifier: $email_id})\n",
    "            MERGE (parent)-[:FORWARDED_MESSAGE]->(child)\n",
    "            \"\"\",\n",
    "            parent_id=parent_email_id,\n",
    "            email_id=email_id,\n",
    "        )\n",
    "\n",
    "    # Sender\n",
    "    sender = email_obj.get(\"sender\")\n",
    "    if isinstance(sender, dict):\n",
    "        sender_key = upsert_person(tx, sender)\n",
    "        if sender_key:\n",
    "            tx.run(\n",
    "                \"\"\"\n",
    "                MATCH (e:Email {identifier: $email_id})\n",
    "                MATCH (p:Person {key: $sender_key})\n",
    "                MERGE (p)-[:SENT]->(e)\n",
    "                \"\"\",\n",
    "                email_id=email_id,\n",
    "                sender_key=sender_key,\n",
    "            )\n",
    "\n",
    "    # Recipients\n",
    "    for rcpt in email_obj.get(\"recipient\") or []:\n",
    "        if isinstance(rcpt, dict):\n",
    "            rcpt_key = upsert_person(tx, rcpt)\n",
    "            if rcpt_key:\n",
    "                tx.run(\n",
    "                    \"\"\"\n",
    "                    MATCH (e:Email {identifier: $email_id})\n",
    "                    MATCH (p:Person {key: $rcpt_key})\n",
    "                    MERGE (e)-[:SENT_TO]->(p)\n",
    "                    \"\"\",\n",
    "                    email_id=email_id,\n",
    "                    rcpt_key=rcpt_key,\n",
    "                )\n",
    "\n",
    "    # Mentions\n",
    "    for mention in email_obj.get(\"mentions\") or []:\n",
    "        if isinstance(mention, dict):\n",
    "            upsert_mention_for_email(tx, email_id, mention)\n",
    "\n",
    "    # Attachments\n",
    "    for att in email_obj.get(\"attachments\") or []:\n",
    "        if isinstance(att, dict):\n",
    "            upsert_attachment(tx, email_id, case_id, att)\n",
    "\n",
    "    # Forwarded / nested\n",
    "    fwd = email_obj.get(\"forwardedMessage\")\n",
    "    if isinstance(fwd, dict):\n",
    "        upsert_email_recursive(tx, case_id, fwd, parent_email_id=email_id)\n",
    "\n",
    "    # mentionsEmail\n",
    "    for me in email_obj.get(\"mentionsEmail\") or []:\n",
    "        if isinstance(me, dict):\n",
    "            ref_id = me.get(\"identifier\")\n",
    "            if ref_id:\n",
    "                tx.run(\n",
    "                    \"\"\"\n",
    "                    MERGE (ref:Email {identifier: $ref_id})\n",
    "                    \"\"\",\n",
    "                    ref_id=ref_id,\n",
    "                )\n",
    "                tx.run(\n",
    "                    \"\"\"\n",
    "                    MATCH (e:Email {identifier: $email_id})\n",
    "                    MATCH (ref:Email {identifier: $ref_id})\n",
    "                    MERGE (e)-[:MENTIONS_EMAIL]->(ref)\n",
    "                    \"\"\",\n",
    "                    email_id=email_id,\n",
    "                    ref_id=ref_id,\n",
    "                )\n",
    "\n",
    "\n",
    "# ----------------- Main import with logging & error handling ----------------- #\n",
    "\n",
    "def import_jsonl_to_neo4j(\n",
    "    jsonl_path: str,\n",
    "    uri: str,\n",
    "    user: str,\n",
    "    password: str,\n",
    "    log_every: int = 25,\n",
    "):\n",
    "    \"\"\"\n",
    "    Import JSONL case/email schemas into Neo4j with:\n",
    "      - progress logging every `log_every` lines\n",
    "      - per-line try/except so a bad record doesn't kill the whole run\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    total_lines = 0\n",
    "    success_cases = 0\n",
    "    skipped_lines = 0\n",
    "    failed_cases = 0\n",
    "\n",
    "    with driver.session() as session:\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            start_time = time.time()\n",
    "            for line_no, line in enumerate(f, start=1):\n",
    "                total_lines += 1\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    skipped_lines += 1\n",
    "                    continue\n",
    "\n",
    "                # Progress log\n",
    "                if line_no % log_every == 0:\n",
    "                    print(f\"[INFO] Processing line {line_no}... (success={success_cases}, failed={failed_cases}, skipped={skipped_lines})\")\n",
    "                    print('\\t took', time.time() - start_time, 'seconds')\n",
    "\n",
    "                try:\n",
    "                    wrapper = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"[WARN] Skipping line {line_no}: invalid JSON wrapper ({e})\")\n",
    "                    skipped_lines += 1\n",
    "                    continue\n",
    "\n",
    "                output_raw = wrapper.get(\"output\")\n",
    "                if not output_raw:\n",
    "                    print(f\"[WARN] Skipping line {line_no}: no 'output' field\")\n",
    "                    skipped_lines += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    case_obj = json.loads(output_raw)\n",
    "                except json.JSONDecodeError:\n",
    "                    if isinstance(output_raw, dict):\n",
    "                        case_obj = output_raw\n",
    "                    else:\n",
    "                        print(f\"[WARN] Skipping line {line_no}: invalid 'output' JSON\")\n",
    "                        skipped_lines += 1\n",
    "                        continue\n",
    "\n",
    "                case_id = case_obj.get(\"identifier\")\n",
    "\n",
    "                # Wrap the write in try/except so a single bad case doesn't kill everything\n",
    "                try:\n",
    "                    def work(tx):\n",
    "                        upsert_case(tx, case_obj)\n",
    "\n",
    "                    session.execute_write(work)\n",
    "                    success_cases += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    failed_cases += 1\n",
    "                    print(f\"[ERROR] Failed to import case on line {line_no} (case_id={case_id!r}): {type(e).__name__}: {e}\")\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    print(\"\\n=== Import summary ===\")\n",
    "    print(f\"Total lines read:     {total_lines}\")\n",
    "    print(f\"Successful cases:     {success_cases}\")\n",
    "    print(f\"Failed cases:         {failed_cases}\")\n",
    "    print(f\"Skipped lines:        {skipped_lines}\")\n",
    "    print('Runtime (s):          ', time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e7dd0",
   "metadata": {},
   "source": [
    "## continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_constraints(uri=NEO4J_URI, user=NEO4J_USER, password=NEO4J_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_path = 'enriched_output.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351247eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing line 50... (success=49, failed=0, skipped=0)\n",
      "[INFO] Processing line 100... (success=99, failed=0, skipped=0)\n",
      "[INFO] Processing line 150... (success=149, failed=0, skipped=0)\n",
      "[INFO] Processing line 200... (success=199, failed=0, skipped=0)\n",
      "[INFO] Processing line 250... (success=249, failed=0, skipped=0)\n",
      "[INFO] Processing line 300... (success=299, failed=0, skipped=0)\n",
      "[INFO] Processing line 350... (success=349, failed=0, skipped=0)\n",
      "[INFO] Processing line 400... (success=399, failed=0, skipped=0)\n",
      "[INFO] Processing line 450... (success=449, failed=0, skipped=0)\n",
      "[INFO] Processing line 500... (success=499, failed=0, skipped=0)\n",
      "[INFO] Processing line 550... (success=549, failed=0, skipped=0)\n",
      "[INFO] Processing line 600... (success=599, failed=0, skipped=0)\n",
      "[INFO] Processing line 650... (success=649, failed=0, skipped=0)\n",
      "\n",
      "=== Import summary ===\n",
      "Total lines read:     652\n",
      "Successful cases:     652\n",
      "Failed cases:         0\n",
      "Skipped lines:        0\n"
     ]
    }
   ],
   "source": [
    "import_jsonl_to_neo4j(\n",
    "    jsonl_path=jsonl_path,\n",
    "    uri=NEO4J_URI,\n",
    "    user=\"neo4j\",\n",
    "    password=NEO4J_PASS,\n",
    "    log_every=50 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete entire graph\n",
    "with driver.session() as session:\n",
    "    cypher_query = session.run(\"MATCH (n) DETACH DELETE n;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5571123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d40fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
